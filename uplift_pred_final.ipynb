{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zyBhk4CFQ8z"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import dask.dataframe as dd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# configurations for this notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi9EM9_6rizP"
   },
   "outputs": [],
   "source": [
    "# Dataset too big to be imported with pandas, so we can use dask instead\n",
    "df = dd.read_csv('./data/65d4f0fcb8af9_amex_campus_challenge_train_3.csv')\n",
    "\n",
    "nrows = df.shape[0].compute()\n",
    "ncols = df.shape[1]\n",
    "\n",
    "print(\"The dataset has \", nrows, \"rows and \", ncols, \"columns.\")\n",
    "\n",
    "# Save dataframe as parquet for greater efficiency\n",
    "df.to_parquet('./data/train_data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S_b2pwlqGvC"
   },
   "outputs": [],
   "source": [
    "# Code provided by amex. Do not edit!\n",
    "\n",
    "### Scoring function for participating teams :\n",
    "def incr_act_top10(input_df: pd.DataFrame,\n",
    "                   pred_col: str,\n",
    "                   cm_key='customer',\n",
    "                   treated_col='ind_recommended',\n",
    "                   actual_col='activation') -> float:\n",
    "    '''\n",
    "    Function that returns the incremental activation score for the AMEX Singapore Hackathon 2024\n",
    "\n",
    "    input_df : pandas Dataframe which has customer, ind_recommended, activation and pred_col\n",
    "    pred_col : name of your prediction score variable\n",
    "    cm_key : customer unique ID (do not change)\n",
    "    treated_col : indicator variable whether a merchant was recommended\n",
    "    actual_col : whether a CM had transacted at a given merchant (target variable)\n",
    "\n",
    "    Returns - incremental activation\n",
    "    '''\n",
    "\n",
    "\t# for correcting variable types\n",
    "    input_df[[treated_col, actual_col, pred_col]] = input_df[[treated_col, actual_col, pred_col]].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    input_df['rank_per_cm1'] = input_df.groupby(cm_key)[pred_col].rank(method='first', ascending=False)\n",
    "\n",
    "    input_df = input_df.loc[input_df.rank_per_cm1 <= 10,:]\n",
    "\n",
    "    agg_df = input_df.groupby(treated_col,as_index=False).agg({actual_col:'mean'})\n",
    "    agg_df.columns = [treated_col,'avg_30d_act']\n",
    "\n",
    "    print(agg_df)\n",
    "    recommended_avg_30d_act = float(agg_df.loc[agg_df[treated_col]==1,'avg_30d_act'])\n",
    "    not_recommended_avg_30d_act = float(agg_df.loc[agg_df[treated_col]==0,'avg_30d_act'])\n",
    "\n",
    "\n",
    "    return (recommended_avg_30d_act-not_recommended_avg_30d_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "GoMhJ3hM3dEl",
    "outputId": "b6efb5cf-18bb-4e1f-e10e-a885b5fff947"
   },
   "outputs": [],
   "source": [
    "df = dd.read_parquet('./data/train_data.parquet', engine='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h185vcyJ0CSH"
   },
   "outputs": [],
   "source": [
    "# Display list of all columns with at least one NA\n",
    "null_values_per_column = df.isna().sum().compute().sort_values(ascending=False)\n",
    "columns_with_null_values = null_values_per_column[null_values_per_column > 0]\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(columns_with_null_values)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CcIIcohbFnD"
   },
   "outputs": [],
   "source": [
    "# Remove columns with > 50% missingness\n",
    "nrows = df.shape[0].compute()\n",
    "cols_to_drop = df.columns[df.isnull().sum() > nrows/2]\n",
    "df = df.drop(columns = cols_to_drop)\n",
    "\n",
    "# remove rows with > 50% missingness in remaining 35 features (aka keep rows w at least 18 non-null values)\n",
    "df = df.dropna(thresh = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -999 values with NaN\n",
    "df['merchant_spend_06'] = df['merchant_spend_06'].mask(df['merchant_spend_06'] == -999)\n",
    "df['customer_profile_01'] = df['customer_profile_01'].mask(df['customer_profile_01'] == -999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6XeuJr3ayrD"
   },
   "source": [
    "## Missing Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I attempted to use dask for parallelized imputation, but it didn't work due to memory limitations.\n",
    "\n",
    "Imputation strategy: \n",
    "- mean imputation for non-skewed numerical variables;\n",
    "- median imputation for skewed numerical variables;\n",
    "- mode imputation for categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data using the standard iterative way.\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "cols_mean = ['customer_spend_02', 'customer_spend_03', 'merchant_spend_06', 'customer_merchant_03',\n",
    "             'customer_digital_activity_21', 'customer_digital_activity_22', 'merchant_profile_02',\n",
    "             'merchant_spend_09', 'merchant_profile_03', 'customer_digital_activity_01',\n",
    "             'customer_profile_04', 'customer_spend_07']\n",
    "cols_median = ['customer_spend_01', 'customer_industry_spend_01', 'customer_industry_spend_02',\n",
    "               'customer_industry_spend_03', 'customer_industry_spend_04', 'customer_industry_spend_05',\n",
    "               'customer_spend_05', 'customer_spend_06', 'merchant_spend_01', 'merchant_spend_02',\n",
    "               'merchant_spend_03', 'merchant_spend_04', 'merchant_spend_05', 'merchant_spend_07',\n",
    "               'merchant_spend_08', 'customer_profile_01', 'customer_profile_02', 'distance_04',\n",
    "               'merchant_spend_10', 'customer_profile_03', 'customer_digital_activity_02', 'distance_05']\n",
    "cols_mode = ['merchant_profile_01']\n",
    "\n",
    "for col in cols_mean:\n",
    "    df[col] = mean_imputer.fit_transform(df[[col]])\n",
    "\n",
    "for col in cols_median:\n",
    "    df[col] = median_imputer.fit_transform(df[[col]])\n",
    "    \n",
    "for col in cols_mode:\n",
    "    df[col] = mode_imputer.fit_transform(df[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save imputed dataset \n",
    "df.to_parquet('./data/train_data_imputed.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maumlvWFoE8c"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJgMqQXtdJuB"
   },
   "outputs": [],
   "source": [
    "# Import imputed dataset\n",
    "df = pd.read_parquet('./data/train_data_imputed.parquet', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gMqRT9iO4JF"
   },
   "source": [
    "## Data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UihjGiXu5F8u"
   },
   "outputs": [],
   "source": [
    "# Exclude customer and merchant columns\n",
    "df_feats = df.drop(columns = ['customer', 'merchant'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profile report\n",
    "profile = ProfileReport(df_feats, title = 'Profiling Report', minimal = True)\n",
    "profile.to_file(\"profiling report_imputed_minimal.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y3TFkAcOzyU"
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X7_x38tPMwQ"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_mat = df_feats.corr() # as i did this locally as well, this might not run on colab\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(40, 30))\n",
    "\n",
    "# Plot correlation heatmap\n",
    "heatmap = sns.heatmap(corr_mat, vmin=-1, vmax=1, cmap='BrBG', annot = True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)\n",
    "\n",
    "# Export as png\n",
    "fig = heatmap.get_figure()\n",
    "fig.savefig(\"corr_heatmap_annot_v2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQPUObCIOr-H"
   },
   "source": [
    "### Findings from correlation matrix:\n",
    "\n",
    "- `merchant_spend_02` highly correlated with `merchant_spend_08` (0.96)\n",
    "  - merchants' customers and merchants' transactions. makes sense.\n",
    "  - let's keep merchants' transactions as info on merchant customers is probably embedded within it.\n",
    "\n",
    "- `merchant_spend_01` highly correlated with `merchant_spend_03` (0.88)\n",
    "  - dummy variables.\n",
    "  - dropped ms03.\n",
    "\n",
    "- `customer_spend_05`, `customer_profile_01`, and `customer_profile_02` all highly correlated with one another (cp01-cp02: 0.95; cp01-cs05: 0.79, cp02-cs05: 0.78)\n",
    "  - customer amount spent, and the other 2 are dummy variables.\n",
    "  - drop cp01 and cp02.\n",
    "\n",
    "- `customer_industry_spend_02`, `customer_industry_spend_04` and `customer_industry_spend_05` all highly correlated with one another (cis02-cis04: 0.71, cis02-cis05: 0.85, cis04-cis05: 0.83)\n",
    "  - cis04 is customer industry transaction, other 2 are dummy variables.\n",
    "  - drop cis02 and cis05.\n",
    "\n",
    "- `customer_spend_02`, `customer_spend_03`, `customer_spend_06`, `customer_spend_07` all highly correlated with one another (cs02-cs03: 0.77, cs02-cs06: 0.62, cs02-cs07: 0.72, cs03-cs06: 0.7, cs03-cs07: 0.83, cs06-cs07: 0.74)\n",
    "  - cs03 is customer unique merchants, cs06 is customer transactions, cs07 is days with spend. cs02 is dummy.\n",
    "  - drop cs02 as it's dummy thus no meaning to us.\n",
    "  - also drop cs03 and cs07. rationale is that cs06 which is customer transactions probably embeds the info within cs03 and cs07.\n",
    "\n",
    "Moderate correlations:\n",
    "\n",
    "- `merchant_spend_09` with `merchant_profile_03` (0.67)\n",
    "  - both dummy.\n",
    "\n",
    "- `customer_profile_03` with `customer_spend_07` (0.61)\n",
    "  - cs07 is days with spend, cp03 is dummy.\n",
    "  - drop cp03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22YxK4qDOrfg"
   },
   "outputs": [],
   "source": [
    "# Drop cis02, cis05, cs02, cp01, cp02, cp03 as they are highly correlated w other variables,\n",
    "# and they're considered dummy variables w not much meaning for us\n",
    "df = df.drop(columns = ['customer_industry_spend_02', 'customer_industry_spend_05',\n",
    "                        'customer_spend_02', 'customer_profile_03',\n",
    "                        'customer_profile_01', 'customer_profile_02'], axis = 1)\n",
    "\n",
    "# Drop customer_digital_activity_21, customer_digital_activity_22, merchant_spend_03,\n",
    "# merchant_spend_04, merchant_spend_05\n",
    "# as there are MANY zeros and these are dummy variables,\n",
    "# which means we don't know what exactly they represent and the significance of these zeros.\n",
    "# Including these variables may result in biased estimates, so let's drop them.\n",
    "df = df.drop(columns = ['customer_digital_activity_21', 'customer_digital_activity_22',\n",
    "                        'merchant_spend_03', 'merchant_spend_04', 'merchant_spend_05'])\n",
    "\n",
    "# Drop merchant_spend_02 as the info in this column is likely to be embedded within merchant_spend_08\n",
    "df = df.drop(columns = ['merchant_spend_02'], axis = 1)\n",
    "\n",
    "# Drop customer_spend_03 and customer_spend_07 as the info within them is likely to be embedded\n",
    "# within customer_spend_06\n",
    "df = df.drop(columns = ['customer_spend_03', 'customer_spend_07'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RfSACNxPjB6"
   },
   "outputs": [],
   "source": [
    "# Round imputed variables to whole numbers or floats\n",
    "\n",
    "# Rounded to whole numbers\n",
    "vars_int = ['customer_industry_spend_04', 'merchant_spend_06', 'merchant_spend_08',\n",
    "            'merchant_profile_03', 'customer_spend_06', 'merchant_spend_09']\n",
    "\n",
    "for var in vars_int:\n",
    "  df[var] = df[var].round().astype(int)\n",
    "\n",
    "# Rounded to 2dp floats\n",
    "vars_twodp = ['customer_spend_05', 'customer_industry_spend_03', 'merchant_spend_07']\n",
    "\n",
    "for var in vars_twodp:\n",
    "  df[var] = df[var].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv7PJfRmMH4j"
   },
   "outputs": [],
   "source": [
    "# Use PCA to reduce groups of dummy variables in same feature category into 1 feature\n",
    "\n",
    "merchant_spend_df = df[['merchant_spend_01', 'merchant_spend_06', 'merchant_spend_09']]\n",
    "merchant_prof_df = df[['merchant_profile_02', 'merchant_profile_03']]\n",
    "distance_df = df[['distance_04', 'distance_05']]\n",
    "dfs_to_transform = {'merchant_spend': merchant_spend_df,\n",
    "                    'merchant_profile': merchant_prof_df,\n",
    "                    'distance': distance_df}\n",
    "\n",
    "def reduce_dimensions(df):\n",
    "  '''\n",
    "  Takes a dataframe and scales it to standard normal,\n",
    "  then uses PCA to reduce columns in the dataframe to 1 column.\n",
    "\n",
    "  Argument:\n",
    "  - dataframe with shape (x, y).\n",
    "  \n",
    "  Output: \n",
    "  - array of shape (x, 1)\n",
    "  '''\n",
    "  # scale df to standard normal\n",
    "  scaler = StandardScaler()\n",
    "  df = scaler.fit_transform(df)\n",
    "\n",
    "  # apply PCA to reduce variables to 1 column\n",
    "  pca = PCA(n_components = 1)\n",
    "  principal_comps = pca.fit_transform(df)\n",
    "\n",
    "  return principal_comps\n",
    "\n",
    "# initialise empty dataframe\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# iteratively run PCA on each dataframe and concat them to the result dataframe\n",
    "for key, value in dfs_to_transform.items():\n",
    "  principal_comps = reduce_dimensions(value)\n",
    "  reduced_df = pd.DataFrame(data = principal_comps, columns=[key])\n",
    "  result = pd.concat([result, reduced_df], axis = 1)\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlfSLL60X_Am"
   },
   "outputs": [],
   "source": [
    "# Replace columns in training dataframe with principal components of each dummy variable category\n",
    "df = df.drop(columns = ['merchant_spend_01', 'merchant_spend_06', 'merchant_spend_09',\n",
    "                        'merchant_profile_02', 'merchant_profile_03', 'distance_04', 'distance_05'], axis = 1).reset_index(drop = True)\n",
    "\n",
    "df = pd.concat([df, result], axis = 1)\n",
    "\n",
    "print(f'Number of features remaining = {df.shape[1] - 4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCW9P66HQwkh"
   },
   "outputs": [],
   "source": [
    "# Save partially cleaned dataset to reduce workload next time\n",
    "#df.to_parquet('./data/train_data_partial_cleaned_040324.parquet', engine='pyarrow')\n",
    "\n",
    "# Read in this dataframe\n",
    "df = pd.read_parquet('./data/train_data_partial_cleaned_040324.parquet', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vATPf60HJ_Ap"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZXcJ8wQfqeA"
   },
   "outputs": [],
   "source": [
    "# Log-transform skewed numerical variables with minimum value of >0\n",
    "cols_to_log = ['customer_industry_spend_01', 'customer_industry_spend_03', 'customer_industry_spend_04',\n",
    "               'customer_spend_01', 'customer_spend_05', 'customer_spend_06',\n",
    "               'merchant_spend_07', 'merchant_spend_08', 'merchant_spend_10']\n",
    "\n",
    "for col in cols_to_log:\n",
    "  new_colname = col + '_log'\n",
    "  df[new_colname] = np.log(df[col])\n",
    "\n",
    "df = df.drop(columns = cols_to_log, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize customer_digital_activity_02 into 5 buckets based on percentile\n",
    "\n",
    "def discretize(value):\n",
    "    '''\n",
    "    bucket 1: 0-25 percentile\n",
    "    bucket 2: 25-50 percentile (median)\n",
    "    bucket 3: 50-75 percentile\n",
    "    bucket 4: 75-95 percentile\n",
    "    bucket 5: top 5 percentile\n",
    "    '''\n",
    "    if 0 <= value <= 0.8333:\n",
    "        return 0\n",
    "    elif 0.8333 < value <= 2.1667:\n",
    "        return 1\n",
    "    elif 2.1667 < value <= 5.3333:\n",
    "        return 2\n",
    "    elif 5.3333 < value <= 32.3333:\n",
    "        return 3\n",
    "    else: # value > 32.3333\n",
    "        return 4\n",
    "\n",
    "df['customer_login_discretized'] = df['customer_digital_activity_02'].apply(discretize)\n",
    "df = df.drop('customer_digital_activity_02', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fully cleaned dataset\n",
    "#df.to_parquet('./data/train_data_clean.parquet', engine='pyarrow')\n",
    "\n",
    "# read fully cleaned dataset\n",
    "df = pd.read_parquet('./data/train_data_clean.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Choose uplift model algorithm based on holdout set of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize continuous features \n",
    "# Don't standardize categorical features + PCA features\n",
    "df_feats = df.drop(columns = ['customer', 'merchant', 'ind_recommended', 'activation'], axis = 1)\n",
    "cols_to_norm = [col for col in df_feats if col not in ['merchant_profile_01', 'customer_login_discretized', 'distance', 'merchant_profile', 'merchant_spend']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_feats[cols_to_norm] = scaler.fit_transform(df_feats[cols_to_norm])\n",
    "\n",
    "# Declare `merchant_profile_01` and `customer_login_discretized` as categorical data for xgboost\n",
    "df_feats['merchant_profile_01'] = df_feats[\"merchant_profile_01\"].astype(\"category\")\n",
    "df_feats['customer_login_discretized'] = df_feats[\"customer_login_discretized\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Transformation\n",
    "\n",
    "For more info on this model, visit https://www.uplift-modeling.com/en/latest/user_guide/models/revert_label.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.models import ClassTransformation\n",
    "\n",
    "estimator = XGBClassifier(enable_categorical = True)\n",
    "\n",
    "ct = ClassTransformation(estimator)\n",
    "ct = ct.fit(X_train, y_train, treat_train)\n",
    "\n",
    "uplift_ct = ct.predict(X_val)\n",
    "\n",
    "val_results_df['pred_col'] = uplift_ct\n",
    "\n",
    "# Calculate IAR \n",
    "ct_score = incr_act_top10(input_df = val_results_df, pred_col = 'pred_col')\n",
    "print(f\"Incremental activation rate is {ct_score}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_train, y_train and treat_train dataframes\n",
    "X_train = df_feats\n",
    "y_train = df['activation']\n",
    "treat_train = df['ind_recommended']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "from sklift.models import ClassTransformation\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'lambda': np.power(10., np.arange(0,3)),\n",
    "    'alpha': np.power(10., np.arange(0,3))\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5) # 5-fold CV\n",
    "\n",
    "# Initialise best_params and best_score variables\n",
    "best_params = None\n",
    "best_score = -np.inf\n",
    "\n",
    "for max_depth in param_grid['max_depth']:\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            for lmbda in param_grid['lambda']:\n",
    "                for alpha in param_grid['alpha']:\n",
    "                    scores = []\n",
    "                    \n",
    "                    for train_index, val_index in skf.split(X_train, treat_train):\n",
    "                        \n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold = y_train.iloc[train_index]\n",
    "                        treat_train_fold = treat_train.iloc[train_index]\n",
    "                        val_results_df = df.loc[val_index, ['customer', 'ind_recommended', 'activation']]\n",
    "                        \n",
    "                        xgb_est_params = {\n",
    "                            'max_depth': max_depth,\n",
    "                            'learning_rate': learning_rate, \n",
    "                            'n_estimators': n_estimators,\n",
    "                            'lambda': lmbda,\n",
    "                            'alpha': alpha,   \n",
    "                            'seed': 42,\n",
    "                            'enable_categorical': True\n",
    "                        }\n",
    "                        \n",
    "                        estimator = XGBClassifier(**xgb_est_params)\n",
    "\n",
    "                        # fit the model and make predictions\n",
    "                        ct = ClassTransformation(estimator)\n",
    "                        ct = ct.fit(X_train_fold, y_train_fold, treat_train_fold)\n",
    "                        uplift_ct = ct.predict(X_val_fold)\n",
    "                        val_results_df['pred_col'] = uplift_ct\n",
    "\n",
    "                        # calculate IAR\n",
    "                        ct_score = incr_act_top10(input_df=val_results_df, pred_col='pred_col')\n",
    "                        scores.append(ct_score)\n",
    "                    \n",
    "                    # calculate average scores across all 5 folds    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    # update best score/params if this is the best model so far\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = xgb_est_params\n",
    "                    \n",
    "                    print(f\"Parameters: max_depth={max_depth}, learning_rate={learning_rate}, n_estimators={n_estimators}, lambda={lmbda}, alpha={alpha}, score={avg_score}\")\n",
    "\n",
    "# Print out the best parameters and IAR\n",
    "print(f\"Best Incremental Activation Rate: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFECV using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.models import ClassTransformation\n",
    "\n",
    "scores = []\n",
    "eliminated_features = []\n",
    "features = X_train.columns.tolist()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5) # 5-fold CV\n",
    "\n",
    "while len(features) > 1:\n",
    "    \n",
    "    current_scores = []\n",
    "    feature_importances = np.zeros(len(features))\n",
    "    \n",
    "    for train_index, val_index in skf.split(X_train[features], treat_train):\n",
    "                            \n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index][features], X_train.iloc[val_index][features]\n",
    "        y_train_fold = y_train.iloc[train_index]\n",
    "        treat_train_fold = treat_train.iloc[train_index]\n",
    "        val_results_df = df.loc[val_index, ['customer', 'ind_recommended', 'activation']]\n",
    "        \n",
    "        xgb_est_params = {\n",
    "            'max_depth':4,\n",
    "            'learning_rate': 0.2, \n",
    "            'n_estimators': 200,\n",
    "            'lambda': 100,\n",
    "            'alpha': 10,\n",
    "            'n_jobs': -1,\n",
    "            'seed': 42,\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "        \n",
    "        estimator = XGBClassifier(**xgb_est_params)\n",
    "        ct = ClassTransformation(estimator)\n",
    "        ct = ct.fit(X_train_fold, y_train_fold, treat_train_fold)\n",
    "        uplift_ct = ct.predict(X_val_fold)\n",
    "        val_results_df['pred_col'] = uplift_ct\n",
    "\n",
    "        # Calculate IAR and update best score/params if this is the best model so far\n",
    "        ct_score = incr_act_top10(input_df=val_results_df, pred_col='pred_col')\n",
    "        current_scores.append(ct_score)\n",
    "        feature_importances += estimator.feature_importances_ / skf.n_splits\n",
    "\n",
    "    # calculate average scores across all 5 folds    \n",
    "    avg_score = np.mean(current_scores)\n",
    "    scores.append((features, avg_score))\n",
    "    \n",
    "    # eliminate the least important feature\n",
    "    least_important_feature_index = np.argmin(feature_importances)\n",
    "    eliminated_features.append(features.pop(least_important_feature_index))\n",
    "    \n",
    "    print(f\"features remaining: {features}; features eliminated: {eliminated_features}\")\n",
    "\n",
    "best_features, best_score = max(scores, key=lambda x: x[1])\n",
    "print(\"Best Features:\", best_features)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: \n",
    "- max depth: 4\n",
    "- learning rate: 0.2\n",
    "- no. of estimators: 200\n",
    "- lambda: 100\n",
    "- alpha: 10\n",
    "\n",
    "Best features:\n",
    "- `merchant_profile_01`\n",
    "- `customer_login_discretized`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Train Class Transformation model on whole training dataset and test on eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleaned eval dataset\n",
    "test_df = pd.read_parquet('./data/eval_data_clean.parquet', engine = 'pyarrow')\n",
    "\n",
    "X_test = test_df.loc[:, ['merchant_profile_01', 'customer_login_discretized']]\n",
    "\n",
    "# declare `merchant_profile_01` and `customer_login_discretized` as categorical data for xgboost\n",
    "X_test['merchant_profile_01'] = X_test[\"merchant_profile_01\"].astype(\"category\")\n",
    "X_test['customer_login_discretized'] = X_test[\"customer_login_discretized\"].astype(\"category\")\n",
    "\n",
    "# prepare results dataframe\n",
    "result = test_df[['customer', 'merchant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on full training data \n",
    "from sklift.models import ClassTransformation\n",
    "\n",
    "X_train_full = df.loc[:, ['merchant_profile_01', 'customer_login_discretized']]\n",
    "y_train_full = df.loc[:, 'activation']\n",
    "treat_train_full = df.loc[:, 'ind_recommended']\n",
    "\n",
    "xgb_est_params = {\n",
    "    'max_depth':4,\n",
    "    'learning_rate': 0.2, \n",
    "    'n_estimators': 200,\n",
    "    'lambda': 100,\n",
    "    'alpha': 10,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "estimator = XGBClassifier(**xgb_est_params)\n",
    "\n",
    "ct = ClassTransformation(estimator)\n",
    "ct = ct.fit(X_train_full, y_train_full, treat_train_full)\n",
    "\n",
    "# use model to predict test data\n",
    "uplift_ct_test = ct.predict(X_test)\n",
    "result['predicted_score'] = uplift_ct_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export result dataframe to CSV\n",
    "result.to_csv('./output/submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
